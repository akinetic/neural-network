# Modelo de Regresi贸n Lineal Segmentada (MRLS)

> Este proyecto implementa el Modelo de Regresi贸n Lineal Segmentada (MRLS), una alternativa a las redes neuronales artificiales (RNA) tradicionales. El MRLS modela conjuntos de datos con funciones lineales a trozos, utilizando un proceso de **compresi贸n neuronal** para reducir la complejidad sin comprometer la precisi贸n m谩s all谩 de una tolerancia definida por el usuario.

El n煤cleo de la soluci贸n es el algoritmo de compresi贸n, que transforma un conjunto de datos desordenado (`DataFrame` / `X, Y`) en un diccionario final, altamente optimizado, listo para realizar predicciones.

## Estructura del Proyecto

* **`mrls-logos.py`**: Contiene la implementaci贸n completa del proceso de entrenamiento (Creaci贸n, Optimizaci贸n, Compresi贸n) y la funci贸n de predicci贸n (`predict`). Este c贸digo genera el diccionario MRLS final que se consume en la web.
* **`index.html`**: Implementaci贸n de la visualizaci贸n en D3.js y JavaScript Vanilla, que muestra el conjunto de datos, la curva de predicci贸n del MRLS (la funci贸n lineal a trozos) y permite interactuar con la funci贸n de predicci贸n en tiempo real.

---

##  Arquitectura del MRLS: El Proceso de Entrenamiento (Compresi贸n)

El entrenamiento del MRLS se logra a trav茅s de cuatro secciones principales, implementadas secuencialmente en `mrls-logos.py`:

### 1. Creaci贸n del Diccionario Base (Secci贸n I y II)

El MRLS es un modelo **no iterativo** (Entrenamiento Instant谩neo). El "entrenamiento" comienza ordenando el conjunto de datos de entrada (`X, Y`) de menor a mayor valor de `X`. Esta ordenaci贸n transforma el `DataFrame` inicial en la estructura fundamental del MRLS: un diccionario donde cada punto `(X, Y)` est谩 indexado por su valor `X`.

**Ejemplo de Conjunto de Entrada (Input Set):**

Para demostrar el proceso, usamos el siguiente conjunto de datos desordenado (Entrada $X$, Salida $Y$):

```
[-6.00,-6.00]
[+2.00,+4.00]
[-8.00,-4.00]
[+0.00,+0.00]
[+4.00,+10.0]
[-4.00,-6.00]
[+6.00,+18.0]
[-5.00,-6.01]
[+3.00,+7.00]
[-2.00,-4.00]
```
Una vez ordenado por $X$, este se convierte en el **Diccionario Base**:

```
// Diccionario Base (Ordenado por X)
[-8.00,-4.00]
[-6.00,-6.00]
[-5.00,-6.01]
[-4.00,-6.00]
[-2.00,-4.00]
[+0.00,+0.00]
[+2.00,+4.00]
[+3.00,+7.00]
[+4.00,+10.0]
[+6.00,+18.0]
```

### 2. Optimizaci贸n (Secci贸n III)

A partir del diccionario base ordenado, se calcula la funci贸n lineal que conecta cada par de puntos adyacentes $(x_1, y_1)$ y $(x_2, y_2)$. Este paso transforma los datos $(X, Y)$ en los par谩metros del segmento:

* **Pendiente (P)**: Representa el **Peso** (`W`) del segmento.
    $$P = \frac{y_2 - y_1}{x_2 - x_1}$$
* **Ordenada al Origen (O)**: Representa el **Sesgo** (`B`) del segmento.
    $$O = y_1 - P \cdot x_1$$

El resultado es un **Diccionario Optimizado** donde cada clave $X_n$ (el inicio del segmento) almacena la tupla $(P, O)$. Este es el conocimiento expl铆cito del modelo.

**Ejemplo de Diccionario Optimizado (Pesos y Sesgos):**

```
// Diccionario Optimizado (Pesos y Sesgos)
[-8.00] (-1.00,-12.0)
[-6.00] (-0.01,-6.06)
[-5.00] (+0.01,-5.96)
[-4.00] (+1.00,-2.00)
[-2.00] (+2.00,+0.00)
[+0.00] (+2.00,+0.00)
[+2.00] (+3.00,-2.00)
[+3.00] (+3.00,-2.00)
[+4.00] (+4.00,-6.00)
```

### 3. Compresi贸n sin P茅rdida (Invarianza Geom茅trica - Secci贸n IV)

Este paso elimina la redundancia geom茅trica del modelo. Si tres puntos consecutivos $(X_{n-1}, X_n, X_{n+1})$ se encuentran sobre la misma l铆nea recta, el punto intermedio $X_n$ se considera redundante.

* **Criterio:** Si $\text{Pendiente}(X_{n-1}) \approx \text{Pendiente}(X_n)$, se elimina el punto $X_n$ del diccionario.
* **Resultado:** Se eliminan "neuronas" intermedias que no contribuyen a un cambio en la direcci贸n de la curva, logrando una compresi贸n del diccionario **sin p茅rdida** de informaci贸n geom茅trica.

**Ejemplo de Compresi贸n sin P茅rdida:**

Se eliminan `[+0.00]` y `[+3.00]` por redundancia de Pendiente, quedando:
```
// Diccionario Optimizado (Compresi贸n sin P茅rdida)
[-8.00] (-1.00,-12.0)
[-6.00] (-0.01,-6.06)
[-5.00] (+0.01,-5.96)
[-4.00] (+1.00,-2.00)
[-2.00] (+2.00,+0.00)
[+2.00] (+3.00,-2.00)
[+4.00] (+4.00,-6.00)
```

### 4. Compresi贸n con P茅rdida (Criterio Humano - Secci贸n V)

Este es el paso de mayor compresi贸n, donde se aplica un **criterio humano** (la tolerancia $\epsilon$) para eliminar puntos cuya contribuci贸n al error global es inferior a un umbral predefinido.

* **Tolerancia ($\epsilon$):** Un valor de error m谩ximo aceptable (por ejemplo, $0.03$).
* **Criterio de Permanencia:** El punto $X_{\text{actual}}$ se mantiene si el error absoluto al interpolar entre sus vecinos es superior a $\epsilon$.

$$\text{Error} = | Y_{\text{true}} - Y_{\text{hat}} |$$

Si $\text{Error} > \epsilon$, el punto se mantiene. Si $\text{Error} \le \epsilon$, se elimina (compresi贸n con p茅rdida).

**Ejemplo de Compresi贸n con P茅rdida Final ($\epsilon=0.03$):**

Se elimina `[-5.00]` al tener un error de $0.01 \le 0.03$ al ser interpolado entre `[-6.00]` y `[-4.00]`.

```
// Diccionario Optimizado (Compresi贸n con P茅rdida Final)
[-8.00] (-1.00,-12.0)
[-6.00] (+0.00,-6.00) // Par谩metros ajustados por la interpolaci贸n
[-4.00] (+1.00,-2.00)
[-2.00] (+2.00,+0.00)
[+2.00] (+3.00,-2.00)
[+4.00] (+4.00,-6.00)
```

---

## 5. Extensiones y Propiedades Operacionales del MRLS

La naturaleza modular de los segmentos del MRLS le otorga propiedades operacionales que lo distinguen de los modelos de redes neuronales iterativas:

### 5.1 Modularidad e Intercambio en Caliente (Hot Swapping)
Dado que cada segmento es aut贸nomo y no interact煤a con los pesos de otros segmentos, el MRLS permite la **Modificaci贸n en Caliente**. Esto significa que se puede actualizar, optimizar o a帽adir un nuevo conjunto de datos en un sector espec铆fico del diccionario **en tiempo real**, sin interrumpir la operaci贸n de inferencia del resto de la red.

### 5.2 Activaci贸n No Lineal y Compresi贸n Multimodal
El proceso de compresi贸n puede extenderse para reemplazar localmente un conjunto de m煤ltiples segmentos lineales por una 煤nica funci贸n de orden superior (ej. cuadr谩tica o exponencial), siempre que el error de sustituci贸n se mantenga dentro de la tolerancia ($\epsilon$). Esto genera una **Compresi贸n Multimodal** y compacta a煤n m谩s la arquitectura.

### 5.3 Caja Transparente (Interpretabilidad Total)
El MRLS es un modelo de "caja transparente". Almacena el conocimiento de forma expl铆cita (Pendiente $P$ y Ordenada $O$ para cada segmento). Esto permite una trazabilidad completa de cada predicci贸n y es ideal para entornos que requieren alta interpretabilidad y auditor铆a.

---

##  Predicci贸n y Generalizaci贸n (Secci贸n VII)

La funci贸n `predict(X)` utiliza el diccionario MRLS final y comprimido.

1.  **B煤squeda del Segmento Activo:** Para una nueva entrada $X$, el modelo encuentra la clave $X_n$ m谩s pr贸xima y menor o igual a $X$ ($X_n \le X$). Esta $X_n$ define el segmento lineal activo $(P, O)$.
2.  **Ecuaci贸n Maestra:** Se aplica la f贸rmula lineal para obtener la predicci贸n $Y_{\text{predicha}}$.

$$Y_{\text{predicha}} = X \cdot P + O$$

### Generalizaci贸n (Extrapolaci贸n)

El MRLS maneja la extrapolaci贸n fuera de los l铆mites de entrenamiento de la siguiente manera:

* **Extrapolaci贸n Segmental (Corta Distancia):** Se extiende el segmento lineal de frontera (el primero o el 煤ltimo) al infinito, utilizando los par谩metros $(P, O)$ del segmento m谩s cercano al l铆mite.
* **Proyecci贸n Zonal (Metaprogresi贸n Avanzada):** En modelos avanzados, el MRLS puede analizar la progresi贸n de los Pesos ($P$) y Sesgos ($O$) cerca de los l铆mites para detectar patrones de orden superior. Esto permite proyectar el siguiente segmento con base en el **patr贸n global de la red**, ofreciendo una extrapolaci贸n de larga distancia potencialmente m谩s precisa.

---

## IX. Bibliograf铆a Conceptual

Las siguientes referencias conceptuales inspiran o contrastan con los principios fundamentales del Modelo de Regresi贸n Lineal Segmentada (MRLS):

1.  Regresi贸n Segmentada y Ajuste de Curvas: Trabajos sobre la aproximaci贸n de funciones complejas mediante modelos de regresi贸n definidos por tramos.
2.  Cuantizaci贸n y Compresi贸n de Modelos: T茅cnicas orientadas a reducir el tama帽o de los modelos neuronales para implementaci贸n en hardware con restricciones de memoria.
3.  Modelos de Caja Blanca (Interpretabilidad): Estudios sobre la trazabilidad y la comprensi贸n de las decisiones de un modelo de predicci贸n.
4.  Modularidad y Arquitecturas Desacopladas: Principios de dise帽o de software que permiten la modificaci贸n local sin efectos colaterales.
5.  Procesos de Entrenamiento y Algoritmos de Optimizaci贸n: Conceptos relacionados con la eficiencia de capacitaci贸n y el entrenamiento no iterativo.
6.  Gesti贸n de Datos Dispersos y Outliers: T茅cnicas para garantizar la robustez del modelo frente a puntos aislados o inconsistencias en los datos de entrada.
7.  Sistemas de Memoria Asociativa: El Diccionario Optimizado como una forma de estructura de datos eficiente para el almacenamiento y recuperaci贸n r谩pida de patrones.
8.  Dise帽o de Sistemas Tolerantes a Fallos: Principios que permiten la modificaci贸n o actualizaci贸n de componentes (Modificaci贸n en Caliente) sin interrupci贸n del servicio global.
9.  Teor铆a de Series Temporales: Trabajos sobre la detecci贸n de patrones de progresi贸n (Metaprogresi贸n) para realizar extrapolaciones de largo alcance.
10.  

---

Archivos del Proyecto

[readme-demo](https://akinetic.github.io/neural-network) : La aplicaci贸n web para la visualizaci贸n.

[slrm-logos.py](../slrm-logos.py) : 
El c贸digo fuente principal que contiene la l贸gica de entrenamiento y predicci贸n (V5.10b).

[slrm_manual.html](https://akinetic.github.io/neural-network/slrm_manual.html)  : (Contenido de este archivo): Este manual t茅cnico (V5.10b).

[slrm_visualizer.html](https://akinetic.github.io/neural-network/slrm_visualizer.html) : La aplicaci贸n web para la visualizaci贸n (Utiliza l贸gica anterior - No actualizado).

---

Authors

Alex Kinetic and Logos

Project under MIT License
